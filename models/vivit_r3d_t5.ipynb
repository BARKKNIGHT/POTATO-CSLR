{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6863f669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source import complete.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "import kagglehub\n",
    "naneet1_hmmmmmm_path = kagglehub.dataset_download('naneet1/hmmmmmm')\n",
    "\n",
    "print('Data source import complete.')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44bca3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ca011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 08:30:12.441347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749371412.463662     979 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749371412.470376     979 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torchvision\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Config, VivitConfig, VivitModel\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from jiwer import wer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf41de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sequence(sequence, NUM_FRAMES):\n",
    "    columns = 4\n",
    "    rows = (NUM_FRAMES + 1) // (columns)\n",
    "    fig = plt.figure(figsize=(32, (16 // columns) * rows))\n",
    "    gs = gridspec.GridSpec(rows, columns)\n",
    "    for j in range(rows * columns):\n",
    "        plt.subplot(gs[j])\n",
    "        plt.axis(\"off\")\n",
    "        frames = sequence[j].permute(1,2,0).numpy()\n",
    "        frames = frames/ frames.max()\n",
    "        plt.imshow(frames)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd8dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6450ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = \"/kaggle/input/hmmmmmm/phoenix/annotations/manual/train.corpus.csv\"\n",
    "test_csv = \"/kaggle/input/hmmmmmm/phoenix/annotations/manual/test.corpus.csv\"\n",
    "dev_csv = \"/kaggle/input/hmmmmmm/phoenix/annotations/manual/dev.corpus.csv\"\n",
    "\n",
    "train_paths = \"/kaggle/input/hmmmmmm/phoenix/fullFrame-210x260px/train\"\n",
    "test_paths = \"/kaggle/input/hmmmmmm/phoenix/fullFrame-210x260px/test\"\n",
    "dev_paths =  \"/kaggle/input/hmmmmmm/phoenix/fullFrame-210x260px/dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d5285d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2510d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e8e643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = { '<p>':0, '<s>':1, '</s>':2}\n",
    "idx_to_word = ['<p>', '<s>', '</s>']\n",
    "\n",
    "arr_train = np.loadtxt(train_csv, delimiter='|', dtype='str')\n",
    "arr_train = np.delete(arr_train,0,0)\n",
    "arr_test = np.loadtxt(test_csv, delimiter='|', dtype='str')\n",
    "arr_test = np.delete(arr_test,0,0)\n",
    "arr_dev = np.loadtxt(dev_csv, delimiter='|', dtype='str')\n",
    "arr_dev = np.delete(arr_dev,0,0)\n",
    "\n",
    "arr = np.concatenate((arr_train, arr_test, arr_dev), axis=0)\n",
    "\n",
    "for sentence in arr:\n",
    "    for word in sentence[3].split(' '):\n",
    "        if word not in idx_to_word:\n",
    "            idx_to_word.append(word)\n",
    "            word_to_idx[word] = len(idx_to_word)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "240964dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1299, 1299)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_to_word), len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04259044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rwth_phoenix(Dataset):\n",
    "    def __init__(self, csv, data_path, frame_transform, video_transform, input_fps, output_fps, max_frames, stride, word_dict):\n",
    "\n",
    "        temp = np.loadtxt(csv, delimiter='|', dtype='str')\n",
    "        self.csv = np.delete(temp, 0, 0)\n",
    "        self.word_dict = word_dict\n",
    "        self.data_path = data_path\n",
    "\n",
    "        self.frame_transform = frame_transform\n",
    "        self.video_transform = video_transform\n",
    "\n",
    "        self.input_fps = input_fps\n",
    "        self.output_fps = output_fps\n",
    "        self.max_frames = max_frames\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder = self.csv[idx][1].split('/')\n",
    "        label = self.csv[idx][3].split(' ')\n",
    "\n",
    "        words = []\n",
    "        for word in label:\n",
    "            words.append(self.word_dict[word])\n",
    "        label = torch.tensor(words)\n",
    "        image_folder_path = os.path.join(self.data_path, folder[0], folder[1])\n",
    "\n",
    "        images = os.listdir(image_folder_path)\n",
    "        end = len(images)\n",
    "\n",
    "        step = self.input_fps/random.choice(self.output_fps)\n",
    "        image_list = []\n",
    "        if int(end//step +1) <= self.max_frames:\n",
    "            frame_num, num = 0, 0\n",
    "            while frame_num < end:\n",
    "                num+=1\n",
    "                if self.stride and num%self.stride == 0:\n",
    "                    image_list.append(str(int(frame_num))+'a')\n",
    "\n",
    "                else:\n",
    "                    img = Image.open(os.path.join(image_folder_path, images[int(frame_num)]))\n",
    "                    tensor_img = self.frame_transform(img)\n",
    "                    image_list.append(tensor_img)\n",
    "                frame_num += step\n",
    "\n",
    "            c, h, w = image_list[-1].shape\n",
    "            while len(image_list) < self.max_frames:\n",
    "                image_list.append(torch.zeros(c,h,w))\n",
    "\n",
    "            tensor_video = torch.stack(image_list[:self.max_frames])\n",
    "            tensor_video = self.video_transform(tensor_video)\n",
    "\n",
    "        else:\n",
    "            frame_positions = np.linspace(0, end, self.max_frames, endpoint=False)\n",
    "            num = 0\n",
    "            for n in frame_positions:\n",
    "                num+=1\n",
    "                if self.stride and num%self.stride == 0:\n",
    "                    image_list.append(str(int(n))+'a')\n",
    "\n",
    "                else:\n",
    "                    img = Image.open(os.path.join(image_folder_path, images[int(n)]))\n",
    "                    tensor_img = self.frame_transform(img)\n",
    "                    image_list.append(tensor_img)\n",
    "\n",
    "            tensor_video = torch.stack(image_list[:self.max_frames])\n",
    "            tensor_video = self.video_transform(tensor_video)\n",
    "\n",
    "\n",
    "        return tensor_video, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0c4203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomCrop((248,200)),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomRotation((-5,5))\n",
    "])\n",
    "\n",
    "video_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b44498ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224,224)),\n",
    "])\n",
    "\n",
    "video_test_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "612c2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sos_token = 1\n",
    "    eos_token = 2\n",
    "\n",
    "    vid, labels = zip(*batch)\n",
    "    vid = torch.stack(vid)\n",
    "\n",
    "    sequence_lens = [len(label) for label in labels]\n",
    "    max_len = max(sequence_lens) + 2\n",
    "\n",
    "    # Pad labels with SOS and EOS tokens\n",
    "    y_input = torch.full((len(labels), max_len), fill_value=0, dtype=torch.long)\n",
    "    y_target = torch.full((len(labels), max_len), fill_value=0, dtype=torch.long)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        y_input[i, 0] = sos_token\n",
    "        y_input[i, 1:len(label) + 1] = label\n",
    "        y_target[i, 0:len(label)] = label\n",
    "        y_target[i, len(label)] = eos_token\n",
    "\n",
    "    return vid, y_input, y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e540a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = rwth_phoenix(csv=train_csv,\n",
    "                       data_path=train_paths,\n",
    "                        frame_transform=image_transform , video_transform=video_transform, input_fps=25, output_fps=list(range(4,10)), max_frames=max_frames, stride=0, word_dict=word_to_idx)\n",
    "\n",
    "test_dataset = rwth_phoenix(csv=test_csv,\n",
    "                       data_path=test_paths,\n",
    "                        frame_transform=image_test_transform , video_transform=video_test_transform, input_fps=25, output_fps=list(range(4,10)), max_frames=max_frames, stride=0, word_dict=word_to_idx)\n",
    "\n",
    "dev_dataset = rwth_phoenix(csv=dev_csv,\n",
    "                       data_path=dev_paths,\n",
    "                        frame_transform=image_test_transform , video_transform=video_test_transform, input_fps=25, output_fps=list(range(4,10)), max_frames=max_frames, stride=0, word_dict=word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7c8b65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a54a5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn,prefetch_factor=5,num_workers=num_workers, pin_memory=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=batch_size, collate_fn=collate_fn,prefetch_factor=5,num_workers=num_workers, pin_memory=True)\n",
    "dev_dataloader = DataLoader(dataset=dev_dataset, shuffle=False, batch_size=batch_size, collate_fn=collate_fn,prefetch_factor=5,num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38791323",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler('cuda')\n",
    "\n",
    "def decode_token(tensor, idx_to_word=idx_to_word):\n",
    "    n = tensor.shape[0]\n",
    "    text = []\n",
    "    for i in range(n):\n",
    "        st = ''\n",
    "        for token in tensor[i]:\n",
    "            if token == 2:\n",
    "                break\n",
    "            elif token != 1 and token != 0:\n",
    "                st += idx_to_word[token.item()]\n",
    "        text.append(st)\n",
    "    return text\n",
    "\n",
    "def train_step(model, optimizer, dataloader, loss_fn, epoch, device=\"cuda\"):\n",
    "    model.train()\n",
    "    train_loss, total_correct_wer = 0, 0\n",
    "\n",
    "    for X, y_input, y_target in tqdm(dataloader, 'Training'):\n",
    "        X, y_input, y_target = X.to(device), y_input.to(device), y_target.to(device)\n",
    "        with autocast('cuda'):\n",
    "            y_logit = model(X, y_input)\n",
    "\n",
    "            if y_logit.isnan().any():\n",
    "                print(\"ABORT!!!! \\nNan found in y_logit!\")\n",
    "                \n",
    "            loss = loss_fn(y_logit.permute(0,2,1), y_target)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = torch.argmax(y_logit, dim=2)\n",
    "        total_correct_wer += wer(decode_token(y_target), decode_token(y_pred))\n",
    "        del X, y_input, y_target, y_logit, loss, y_pred\n",
    "\n",
    "    acc_wer = (total_correct_wer / len(dataloader)) * 100\n",
    "    avg_loss = train_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch} | Train Loss: {avg_loss:.4f} | Train WER: {acc_wer:.2f}%\")\n",
    "\n",
    "\n",
    "def test_step(model, loss_fn, epoch, dataloader, scheduler, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    test_loss, total_correct_wer = 0, 0\n",
    "\n",
    "    for X, y_input, y_target in tqdm(dataloader, 'Testing'):\n",
    "        X, y_input, y_target = X.to(device), y_input.to(device), y_target.to(device)\n",
    "\n",
    "        with autocast('cuda'):\n",
    "            y_logit = model(X, y_input)\n",
    "\n",
    "            if y_logit.isnan().any():\n",
    "                print(\"ABORT!!!! \\nNan found in y_logit!\")\n",
    "\n",
    "            loss = loss_fn(y_logit.permute(0,2,1), y_target)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        y_pred = torch.argmax(y_logit, dim=2)\n",
    "        total_correct_wer += wer(decode_token(y_target), decode_token(y_pred))\n",
    "        del X, y_input, y_target, y_logit, loss, y_pred\n",
    "\n",
    "    acc_wer = (total_correct_wer / len(dataloader)) * 100\n",
    "    avg_loss = test_loss / len(dataloader)\n",
    "\n",
    "    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch} | Test Loss: {avg_loss:.4f} | Test WER: {acc_wer:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af3e3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViViT_SLR(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vivit_weights = \"google/vivit-b-16x2-kinetics400\",\n",
    "                 t5_weights = \"google-t5/t5-base\",\n",
    "                 vocab_size = 1299,\n",
    "                 batch_first=True,\n",
    "                 num_heads=4,\n",
    "                 pad_token=0,\n",
    "                 sos_token=1,\n",
    "                 eos_token=2,\n",
    "                 residual_ratio=0,\n",
    "                 max_pred=512):\n",
    "\n",
    "        super(ViViT_SLR, self).__init__()\n",
    "\n",
    "        self.vivit_weights = vivit_weights\n",
    "        self.t5_weights = t5_weights\n",
    "        self.vivit = VivitModel.from_pretrained(self.vivit_weights, attn_implementation=\"sdpa\", torch_dtype=torch.float32)\n",
    "        r3d_18 = torchvision.models.video.r3d_18(pretrained=True)\n",
    "        self.r3d_18 = nn.Sequential(\n",
    "            r3d_18.stem,\n",
    "            r3d_18.layer1,\n",
    "            r3d_18.layer2,\n",
    "            r3d_18.layer3,\n",
    "            nn.Conv3d(256, 512, kernel_size=(2, 1, 1), stride=(2, 1, 1))\n",
    "        )\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=768, num_heads=num_heads, batch_first=batch_first)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=640, out_features=768)\n",
    "        )\n",
    "\n",
    "        self.non_linear = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=640, out_features=768)\n",
    "        )\n",
    "        self.linear = nn.Linear(512, 768)\n",
    "        \n",
    "        self.residual_ratio = residual_ratio\n",
    "        \n",
    "        self.normalize = nn.LayerNorm(768)\n",
    "\n",
    "        self.t5 = self.copy_pretrained_T5_weights()\n",
    "\n",
    "        self.pad_token = pad_token\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.max_pred = max_pred\n",
    "\n",
    "        self.fc_out = nn.Linear(768, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(self, img, y_tokens):\n",
    "\n",
    "        \n",
    "        encoded_output1 = self.vivit(img).last_hidden_state\n",
    "        encoded_output2 = self.r3d_18(img.permute(0,2,1,3,4))\n",
    "        encoded_output2 = encoded_output2.permute(0,2,3,4,1)\n",
    "        encoded_output2 = encoded_output2.view(-1, 28*28*4,512)\n",
    "        encoded_output2 = self.residual_ratio * self.linear(encoded_output2) + (1 - self.residual_ratio) * self.non_linear(encoded_output2)\n",
    "        encoded_output1 = self.normalize(encoded_output1)\n",
    "        encoded_output2 = self.normalize(encoded_output2)\n",
    "\n",
    "        attn_output, _ = self.cross_attn(query=encoded_output1[:, :3136, :], key=encoded_output2[:, :3136, :], value=encoded_output1[:, :3136, :])\n",
    "\n",
    "        # print(\"Encoded Output 1 (ViViT)\")\n",
    "        # print(\"Min:\", encoded_output1.min().item())\n",
    "        # print(\"Max:\", encoded_output1.max().item())\n",
    "        # print(\"Mean:\", encoded_output1.mean().item())\n",
    "        # print(\"Std Dev:\", encoded_output1.std().item())\n",
    "        # print(\"-\" * 30)\n",
    "\n",
    "        # print(\"Encoded Output 2 (r3d_18)\")\n",
    "        # print(\"Min:\", encoded_output2.min().item())\n",
    "        # print(\"Max:\", encoded_output2.max().item())\n",
    "        # print(\"Mean:\", encoded_output2.mean().item())\n",
    "        # print(\"Std Dev:\", encoded_output2.std().item())\n",
    "        # print(\"-\" * 30)\n",
    "\n",
    "        # print(\"Cross Attention Output\")\n",
    "        # print(\"Min:\", attn_output.min().item())\n",
    "        # print(\"Max:\", attn_output.max().item())\n",
    "        # print(\"Mean:\", attn_output.mean().item())\n",
    "        # print(\"Std Dev:\", attn_output.std().item())\n",
    "        # print(\"-\" * 30)\n",
    "\n",
    "        decoder_attention_mask = (y_tokens != self.pad_token).long()\n",
    "        decoded_output = self.t5.decoder(input_ids=y_tokens, encoder_hidden_states=attn_output, attention_mask=decoder_attention_mask).last_hidden_state\n",
    "        logit = self.fc_out(decoded_output)\n",
    "\n",
    "        return logit\n",
    "\n",
    "    def init_weights(self):\n",
    "\n",
    "        # Output Linear\n",
    "        nn.init.xavier_uniform_(self.fc_out.weight)\n",
    "        if self.fc_out.bias is not None:\n",
    "            nn.init.constant_(self.fc_out.bias, 0)\n",
    "\n",
    "    def copy_pretrained_T5_weights(self):\n",
    "\n",
    "        config = T5Config.from_pretrained(self.t5_weights)\n",
    "        config.vocab_size = 1299\n",
    "        new_model = T5ForConditionalGeneration(config)\n",
    "        pretrained_model = T5ForConditionalGeneration.from_pretrained(self.t5_weights)\n",
    "\n",
    "        pretrained_dict = pretrained_model.state_dict()\n",
    "        new_dict = new_model.state_dict()\n",
    "\n",
    "        # Copy only weights that match shape and exclude embeddings & lm_head\n",
    "        filtered_dict = {}\n",
    "        for k, v in pretrained_dict.items():\n",
    "            if (\n",
    "                k in new_dict\n",
    "                and not k.startswith(\"shared\")\n",
    "                and \"embed_tokens\" not in k\n",
    "                and \"lm_head\" not in k\n",
    "                and new_dict[k].shape == v.shape  # ensure shape matches\n",
    "            ):\n",
    "                filtered_dict[k] = v\n",
    "\n",
    "        print(f\"Copying {len(filtered_dict)} weights from pretrained T5 model.\")\n",
    "        new_dict.update(filtered_dict)\n",
    "        new_model.load_state_dict(new_dict)\n",
    "        return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77cd18b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77d074fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying 256 weights from pretrained T5 model.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device = \"cuda\"\n",
    "\n",
    "model = ViViT_SLR().to(device)\n",
    "model = nn.DataParallel(model)\n",
    "# checkpoint = torch.load(f=\"/kaggle/input/vvocr_final/pytorch/default/1/VVOCR.pth\")\n",
    "# new_state_dict = checkpoint[\"model_state_dict\"]\n",
    "# model.load_state_dict(new_state_dict, strict=True)\n",
    "# model = nn.DataParallel(model)\n",
    "# checkpoint = torch.load(f=\"/kaggle/input/vvocr/pytorch/default/1/VVOCR.pth\")\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# new_state_dict = OrderedDict()\n",
    "# for k, v in checkpoint['model_state_dict'].items():\n",
    "#     new_key = k.replace(\"module.\", \"\")  # strip 'module.' prefix\n",
    "#     new_state_dict[new_key] = v\n",
    "# model.load_state_dict(new_state_dict, strict=True)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f18f767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1418 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Training: 100%|██████████| 1418/1418 [33:59<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 5.5855 | Train WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 158/158 [01:22<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Test Loss: 5.3914 | Test WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1418/1418 [33:48<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 5.2782 | Train WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 158/158 [01:25<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Test Loss: 5.2753 | Test WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1418/1418 [33:51<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 5.1155 | Train WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 158/158 [01:23<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Test Loss: 5.1968 | Test WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1418/1418 [33:49<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 5.0310 | Train WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 158/158 [01:22<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Test Loss: 5.1631 | Test WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1418/1418 [33:51<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 4.9643 | Train WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 158/158 [01:26<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Test Loss: 5.1148 | Test WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1418/1418 [33:50<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 4.9029 | Train WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 158/158 [01:22<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Test Loss: 5.1172 | Test WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1418/1418 [33:52<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 4.8278 | Train WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 158/158 [01:23<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Test Loss: 5.0690 | Test WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1418/1418 [34:01<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 4.7821 | Train WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 158/158 [01:22<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Test Loss: 5.0360 | Test WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1418/1418 [33:55<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 4.6881 | Train WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 158/158 [01:22<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Test Loss: 4.8719 | Test WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1418/1418 [33:56<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 4.5455 | Train WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 158/158 [01:22<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Test Loss: 4.8218 | Test WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1418/1418 [33:59<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 4.4111 | Train WER: 99.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 158/158 [01:22<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Test Loss: 4.7783 | Test WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 665/1418 [15:58<18:05,  1.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_979/2546592970.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   train_step(model=model,\n\u001b[0m\u001b[1;32m      6\u001b[0m              \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m              \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_979/1204915652.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, dataloader, loss_fn, epoch, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "for epoch in range(1,epochs):\n",
    "  train_step(model=model,\n",
    "             optimizer=optimizer,\n",
    "             loss_fn=loss_fn,\n",
    "             epoch=epoch,\n",
    "             dataloader=train_dataloader)\n",
    "  test_step(model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            epoch=epoch,\n",
    "            dataloader=test_dataloader,\n",
    "            scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b4a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
